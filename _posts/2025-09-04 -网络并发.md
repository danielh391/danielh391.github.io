---
layout:     post
title:      网络并发
subtitle:   
date:       2025-09-04
author:     danielh
header-img: img/post-bg-rwd.jpg
catalog: true
tags:
    - 网络


---
关于高性能网络编程核心概念的详细说明。我会从基础概念讲起，逐步深入到 Reactor 模式。

---

## 1. 阻塞 I/O (Blocking I/O) - 传统模式

为了理解为什么需要这些高级技术，我们先看传统阻塞 I/O 的问题。

**工作方式**：
- 当线程执行 `read()`, `accept()`, `write()` 等系统调用时，如果数据未就绪（如没有数据可读、客户端未连接、 socket 缓冲区已满），线程会**被挂起（阻塞）**，进入睡眠状态。
- 直到数据就绪，内核才会唤醒线程，操作完成。

**缺点**：
- **一对一模型**：一个线程处理一个连接。成百上千连接需要成百上千线程。
- **资源浪费**：线程阻塞时，占用内存（栈空间）和内核资源，但 CPU 使用率为 0。
- **上下文切换开销**：大量线程导致操作系统在线程间频繁切换，性能急剧下降。

---

## 2. 非阻塞 I/O (Non-blocking I/O)

非阻塞 I/O 是解决上述问题的第一步。

**工作方式**：
- 通过 `fcntl(fd, F_SETFL, O_NONBLOCK)` 将文件描述符（socket）设置为非阻塞模式。
- 当线程执行 `read()`, `accept()` 等调用时，如果数据未就绪，**调用立即返回一个错误码（如 `EAGAIN` 或 `EWOULDBLOCK`）**，而不是阻塞线程。
- 线程可以继续去做其他工作，稍后再来重试（轮询）。

**优点**：
- **线程不会被挂起**，可以持续运行，CPU 时间不被浪费。

**缺点**：
- **轮询（Polling）开销**：线程需要不断地循环检查每个 socket 是否就绪（`while(true) { for(fd in fds) { read(fd); } }`）。这会导致 CPU 空转，利用率 100% 但实际工作效率低下。

**结论**：单纯的非阻塞 I/O 避免了线程阻塞，但引入了昂贵的轮询开销。我们需要一种更高效的方法来知道“哪个 socket 准备好了”。阻塞和非阻塞的区别在于

---

## 3. I/O 多路复用 (I/O Multiplexing)

I/O 多路复用解决了非阻塞 I/O 的轮询问题。它的核心思想是：**“拜托内核帮我们盯着一大堆 socket，一旦其中有一个或多个就绪了，就通知我们。”** 这样应用程序就不需要自己进行低效的轮询了。

### 主要系统调用：

#### a. `select`
- 最古老的实现。程序员告诉内核关心哪些 fd 集合（可读、可写、异常）。
- **缺点**：
  -  fd 集合大小有限制（通常是 1024）。
  -  每次调用都需要将整个 fd 集合从用户态拷贝到内核态，调用返回后又要拷贝回来。
  -  内核和应用程序都需要线性扫描整个 fd 集合以找出就绪的 fd，效率低下。

#### b. `poll`
- 解决了 `select` 的 fd 数量限制问题，但依然需要每次传递 fd 集合和线性扫描。

#### c. `epoll` (Linux) / `kqueue` (BSD)
- **现代高性能网络的基石**。彻底解决了 `select`/`poll` 的性能问题。
- **工作机制**：
  1.  **`epoll_create`**：创建一个 epoll 实例（上下文）。
  2.  **`epoll_ctl`**：**向 epoll 实例注册/修改/删除**需要监控的 fd 及其感兴趣的事件（读、写等）。**这个过程只在初始化时做一次，无需重复拷贝**。
  3.  **`epoll_wait`**：**等待事件发生**。调用时，内核只返回已经就绪的 fd 列表，而不是全部。应用程序无需遍历所有 fd，直接处理这些就绪的 fd 即可。

**I/O 多路复用的优点**：
- **高性能**：可以同时监听数万甚至数十万的连接。
- **高效**：避免了轮询 CPU 空转和大量不必要的系统调用。
- **单线程处理多连接**：一个线程就可以处理所有连接的 I/O 事件。

**经典工作流程（结合非阻塞 I/O）**：
```cpp
// 1. 创建 epoll 实例
int epoll_fd = epoll_create1(0);

// 2. 将 listen_socket 添加到 epoll，监听读事件（新连接）
struct epoll_event event;
event.events = EPOLLIN; // 监听可读事件
event.data.fd = listen_socket;
epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_socket, &event);

// 3. 进入事件循环
while (running) {
    // 4. 等待事件发生。max_events 是最大返回事件数，-1 表示一直阻塞
    int n = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);

    // 5. 处理所有就绪的事件
    for (int i = 0; i < n; i++) {
        if (events[i].data.fd == listen_socket) {
            // 6. 监听socket可读，表示有新连接到来
            int conn_socket = accept4(listen_socket, ..., SOCK_NONBLOCK); // 非阻塞accept
            // 7. 将新连接的 socket 也加入 epoll，监听读事件
            event.events = EPOLLIN | EPOLLET; // 读事件 + 边缘触发模式
            event.data.fd = conn_socket;
            epoll_ctl(epoll_fd, EPOLL_CTL_ADD, conn_socket, &event);
        } else {
            // 8. 客户端连接可读，表示有数据到来
            int client_fd = events[i].data.fd;
            ssize_t count = read(client_fd, buffer, sizeof(buffer));
            if (count > 0) {
                // 处理数据...
                // write(client_fd, response, ...);
            } else if (count == 0) {
                // 对方关闭连接
                close(client_fd);
                epoll_ctl(epoll_fd, EPOLL_CTL_DEL, client_fd, NULL);
            } // else: 错误处理 (EAGAIN 等)
        }
    }
}
```

---

## 4. Reactor 模式

Reactor 模式不是系统调用，而是一种**设计模式**，它基于**事件驱动**，完美地组织和利用了非阻塞 I/O 和 I/O 多路复用技术。

### 核心组件

Reactor 模式通常包含以下五个角色：
1.  **Handle（句柄）**：即文件描述符（socket）。是事件发生的来源。
2.  **Synchronous Event Demultiplexer（同步事件分离器）**：即 `epoll_wait`, `select` 等。用于等待事件的发生。调用者是线程阻塞的。
3.  **Event Handler（事件处理器）**：一个接口，定义了处理事件的方法（如 `handle_read()`, `handle_write()`）。
4.  **Concrete Event Handler（具体事件处理器）**：实现 `Event Handler` 接口。每个 socket 通常对应一个，包含了处理该 socket 事件的业务逻辑。
5.  **Initiation Dispatcher（初始分发器）**：即 **Reactor** 核心。它注册、移除事件处理器，并运行事件循环：使用 Synchronous Event Demultiplexer 等待事件发生，然后分发给对应的 Event Handler。



### Reactor 的工作流程

1.  **注册**：应用程序将 `Concrete Event Handler` 和它感兴趣的**事件**注册到 `Initiation Dispatcher` (Reactor)。
2.  **监听**：`Initiation Dispatcher` 调用 `Synchronous Event Demultiplexer`（如 `epoll_wait`）来等待事件发生。
3.  **通知**：当有事件发生时（如 socket 可读），`Synchronous Event Demultiplexer` 通知 `Initiation Dispatcher`。
4.  **分发**：`Initiation Dispatcher` 根据事件类型（读/写）和发生的 `Handle`（哪个 socket），找到之前注册的 `Concrete Event Handler`。
5.  **回调**：`Initiation Dispatcher` **回调** `Concrete Event Handler` 的相应方法（如 `handle_read()`）。
6.  **处理**：在 `handle_read()` 方法中，执行非阻塞的 `read()` 操作，并处理读取到的数据（业务逻辑）。

### 线程模型变种

- **单 Reactor 单线程**：所有工作（接收、读、写、计算）都在一个线程内完成。模型简单，但无法利用多核，计算会阻塞 I/O。适合业务处理非常快速的场景。
- **单 Reactor 多线程**：Reactor 在单个线程中只负责监听和分发 I/O 事件。具体的业务处理（如计算、数据库查询）被提交到一个**线程池**中执行。这是最常用的模型。
- **主从 Reactor 多线程**：`Main Reactor` 只负责接收新连接，然后将建立好的连接注册到 `Sub Reactor` 上。`Sub Reactor`（通常有多个）负责相应连接的后续 I/O 事件，之后再分发给线程池处理。**Netty、Nginx** 采用此模型，性能极高。

### 总结与关系

| 技术 | 角色 | 解决的问题 |
| :--- | :--- | :--- |
| **非阻塞 I/O** | **基础能力** | 让单个线程可以在 I/O 未就绪时不被阻塞，可以去干别的事。 |
| **I/O 多路复用** | **通知机制** | 高效地通知应用程序“哪些 I/O 操作已经就绪”，避免了低效轮询。 |
| **Reactor 模式** | **设计模式** | **组织和编排**非阻塞 I/O 和 I/O 多路复用技术。通过**回调机制**将网络事件与业务逻辑解耦，形成一个高效、清晰的事件驱动架构。 |

**一句话总结**：**Reactor 模式利用 I/O 多路复用来高效地监听大量非阻塞 I/O 句柄上的事件，并在事件发生时回调预先注册的事件处理器函数。** 这三者结合，构成了现代高性能网络服务器（如 Nginx、Redis、Netty）的基石。
<!--stackedit_data:
eyJoaXN0b3J5IjpbNjY0Njc0NDMwLDIwMzUzNDk0NDksLTI4OD
YwNjAzMF19
-->